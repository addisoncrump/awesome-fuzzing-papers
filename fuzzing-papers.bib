@inproceedings{fuzzbench,
    author = {Metzman, Jonathan and Szekeres, L\'{a}szl\'{o} and Maurice Romain Simon, Laurent and Trevelin Sprabery, Read and Arya, Abhishek},
    title = {{FuzzBench: An Open Fuzzer Benchmarking Platform and Service}},
    year = {2021},
    isbn = {9781450385626},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3468264.3473932},
    doi = {10.1145/3468264.3473932},
    booktitle = {Proceedings of the 29th ACM Joint Meeting on European Software Engineering Conference and Symposium on the Foundations of Software Engineering},
    pages = {1393–1403},
    numpages = {11},
    series = {ESEC/FSE 2021}
}

@inproceedings{evaluating-fuzz-testing,
    author = {Klees, George and Ruef, Andrew and Cooper, Benji and Wei, Shiyi and Hicks, Michael},
    title = {Evaluating Fuzz Testing},
    year = {2018},
    isbn = {9781450356930},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3243734.3243804},
    doi = {10.1145/3243734.3243804},
    abstract = {Fuzz testing has enjoyed great success at discovering security critical bugs in real software. Recently, researchers have devoted significant effort to devising new fuzzing techniques, strategies, and algorithms. Such new ideas are primarily evaluated experimentally so an important question is: What experimental setup is needed to produce trustworthy results? We surveyed the recent research literature and assessed the experimental evaluations carried out by 32 fuzzing papers. We found problems in every evaluation we considered. We then performed our own extensive experimental evaluation using an existing fuzzer. Our results showed that the general problems we found in existing experimental evaluations can indeed translate to actual wrong or misleading assessments. We conclude with some guidelines that we hope will help improve experimental evaluations of fuzz testing algorithms, making reported results more robust.},
    booktitle = {Proceedings of the 2018 ACM SIGSAC Conference on Computer and Communications Security},
    pages = {2123–2138},
    numpages = {16},
    keywords = {evaluation, fuzzing, security},
    location = {Toronto, Canada},
    series = {CCS '18}
}

@inproceedings{symsan,
    author = {Ju Chen and WookHyun Han and Mingjun Yin and Haochen Zeng and Chengyu Song and Byoungyoung Lee and Heng Yin and Insik Shin},
    title = {{SYMSAN}: Time and Space Efficient Concolic Execution via Dynamic Data-flow Analysis},
    booktitle = {31st USENIX Security Symposium (USENIX Security 22)},
    year = {2022},
    isbn = {978-1-939133-31-1},
    address = {Boston, MA},
    pages = {2531--2548},
    url = {https://www.usenix.org/conference/usenixsecurity22/presentation/chen-ju},
    publisher = {USENIX Association},
    month = aug,
}

@inproceedings{codamosa,
    title={CODAMOSA: Escaping Coverage Plateaus in Test Generation with Pre-trained Large Language Models},
    author={Lemieux, Caroline and Inala, Jeevana Priya and Lahiri, Shuvendu K and Sen, Siddhartha},
    booktitle={45th International Conference on Software Engineering, ser. ICSE},
    year={2023}
}
